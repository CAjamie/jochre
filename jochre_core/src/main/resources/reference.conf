jochre {
	locale = null
	
	# If encoding is not provided, the default encoding is used
	# encoding = UTF-8
	
	# A path to a lexicon file or directory listing possible words in the language
	# lexicon = ..
	
		
	image-analyser {
		# The model used to guess letters
		# letter-model = ...
		
		# The model used to perform splits
		# split-model = ...
		
		# The model used to peform merges
		# merge-model = ...
		
		# The number of guesses to retain at each analysis step
		beam-width = 5
		
		# Only retain those guesses whose probability is at least:
		min-outcome-prob = 0.01

		# The average confidence below which a paragraph is considered to be junk,
		# when considering all of its letters.
		junk-threshold = 0.75
	}

	linguistics {
		class = com.joliciel.jochre.lang.DefaultLinguistics
		
		# If this list is not empty, then words must contain these characters only
		# Other words will not be included in training or evaluation
		# Userful for excluding words in foreign alphabets
		# Note that the list will be augmented with punctuation below
		valid-characters = []
		
		punctuation = [
			",", ".", "\"", "!", "?", ")", "(", "*", ";", ":", "-", "—", "%", "/", "\\", "[", "]", "„", "“",
		]
		
		# Any letter in this list is not considered for splitting
		# Should include any valid letters written with two characters, e.g. in languages where accents are normally
		# added as a separate character.
		dual-character-letters = []
	}
	
	segmenter {
		# Should the segmented image be output for further manual analysis
		draw-segmented-image = false
		
		# Should the pixel spread be output for further manual analysis
		draw-pixel-spread = false
	}
	
	word-chooser {
		# An absolute path to the lexicon
		# lexicon = ...
	
		# An adjustment factor for unknown words
		# 0 means unknown words will not be allowed.
		# 1 means unknown words have no downwards adjustment with respect to known words
		# other values means the raw score for unknown words is multiplied by this factor
		unknown-word-factor = 0.75
		
		# Should we adjust at all with respect to word frequency, or should we
		# simply give one score for known words and another for unknown words.
		frequency-adjusted = false
		
		# The log base indicating how much more weight to give to a common word
		# than a rare word, if frequency-adjusted is true. The score =
		# 1 + (ln(1) / ln(frequencyLogBase)); Default value is 10.0, so that a word
		# with a frequency of 10 has twice the weight of frequency of 1, 100 has 3
		# times the weight, etc.
		frequency-log-base = 100.0
	}
	
	boundaries {
		min-prob-for-decision = 0.5
		splitter {
			# The minimum ratio between the shape's width and it's x-height for the shape
			# to even be considered for splitting.
			min-width-ratio = 1.1
			
			# The minimum ratio between the shape's height and it's x-height for the shape
			# to even be considered for splitting.
			min-height-ratio = 1.0
			
			# The beam width indicating the maximum possible decisions to return for each
			# shape (applies recursively as well).
			beam-width = 5
			
			# The maximum recursive depth to search for splits in a single shape. The
			# maximum number of splits = 2^maxDepth.
			max-depth = 2
			
			min-distance-between-splits = 5
			
			# The tolerance of distance between a guessed split and a real split to
			# assume we got the right answer.
			evaluation-tolerance = 4
		}
		
		merger {
			# The maximum ratio between the merged shape candidate's width & x-height to
			# even be considered for merging.
			max-width-ratio = 1.2
			
			# The maximum ratio of the distance between the two inner shapes & the
			# x-height to even be considered for merging.
			max-distance-ratio = 0.15
		}
	}
	
	csv {
		# Which character should separate cells in the CSV files generated
		separator = "\t"
	
		# Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
		#encoding = null
		
		# Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
		#locale = null
	}
}