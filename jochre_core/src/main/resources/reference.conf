jochre {
	locale = null
	
	# If encoding is not provided, the default encoding is used
	# encoding = UTF-8
	
	# A path to a lexicon file or directory listing possible words in the language
	# lexicon = ..
	
		
	image-analyser {
		# The model used to guess letters
		# letter-model = ...
		
		# The model used to perform splits
		# split-model = ...
		
		# The model used to peform merges
		# merge-model = ...
		
		# The number of guesses to retain at each analysis step
		beam-width = 10
		
		# Only retain those guesses whose probability is at least:
		min-outcome-prob = 0.01

		# The average confidence below which a paragraph is considered to be junk,
		# when considering all of its letters.
		junk-threshold = 0.75
	}

	linguistics {
		class = com.joliciel.jochre.lang.DefaultLinguistics
		
		# If this list is not empty, then words must contain these characters only
		# Other words will not be included in training or evaluation
		# Userful for excluding words in foreign alphabets
		# Note that the list will be augmented with punctuation below
		valid-characters = []
		
		punctuation = [
			",", ".", "\"", "!", "?", ")", "(", "*", ";", ":", "-", "—", "%", "/", "\\", "[", "]", "„", "“", "'", "‘", "’", "‛", "“", "”", "„"
		]
		
		# Any letter in this list is not considered for splitting
		# Should include any valid letters written with two characters, e.g. in languages where accents are normally
		# added as a separate character.
		dual-character-letters = []
	}
	
	segmenter {
		# Should the segmented image be output for further manual analysis
		draw-segmented-image = false
		
		# Should the pixel spread be output for further manual analysis
		# The pixel spread is the number of pixels in the image for each brightness value
		draw-pixel-spread = false
		
		# In a greyscale image, 0 indicates pure black, 255 indicates pure white
		# To begin with, we assume that any pixel below the mean pixel brightness is black.
		# However, since there are typically a lot more light pixels than dark ones, this will
		# give a very light definition of black.
		# We therefore apply a percentile of the part of pixel spread below the mean, in order
		# to define a "darker" black.
		
		# This first percentile determines which pixels in the image can define a shape
		# any pixel below this percentile will initially be considered black, and
		# will initially be included in a contiguous shape (a shape consisting of all touching black pixels)
		# Note that some of these shapes will later be removed because too large (images) or too small (specks)
		# Thus, a higher percentile here will result in more shapes initially.
		separation-threshold-percentile = 75
				
		# Once a shape is defined and finalised, it is still sometimes necessary to consider
		# it's component pixels as black or white.
		# Inside a defined shape, a pixel will be considered black if its brightness is below this percentile
		# Thus, a higher percentile here will result in more black areas inside the shape
		black-threshold-percentile = 60
	}
	
	word-chooser {
		# An absolute path to the lexicon
		# lexicon = ...
	
		# An adjustment factor for unknown words
		# 0 means unknown words will not be allowed.
		# 1 means unknown words have no downwards adjustment with respect to known words
		# other values means the raw score for unknown words is multiplied by this factor
		unknown-word-factor = 0.75
		
		# Should we adjust at all with respect to word frequency, or should we
		# simply give one score for known words and another for unknown words.
		frequency-adjusted = false
		
		# The log base indicating how much more weight to give to a common word
		# than a rare word, if frequency-adjusted is true. The score =
		# 1 + (ln(1) / ln(frequencyLogBase)); Default value is 10.0, so that a word
		# with a frequency of 10 has twice the weight of frequency of 1, 100 has 3
		# times the weight, etc.
		frequency-log-base = 100.0
	}
	
	boundaries {		
		# the probability at which a split/merge should be made
		min-prob-for-decision = 0.5
		
		splitter {
			# The minimum ratio between the shape's width and it's x-height for the shape
			# to even be considered for splitting.
			min-width-ratio = 1.1
			
			# The minimum ratio between the shape's height and it's x-height for the shape
			# to even be considered for splitting.
			min-height-ratio = 1.0
			
			# The beam width indicating the maximum possible decisions to return for each
			# shape (applies recursively as well).
			beam-width = 5
			
			# The maximum recursive depth to search for splits in a single shape. The
			# maximum number of splits = 2^maxDepth.
			max-depth = 2
			
			min-distance-between-splits = 5
			
			# The tolerance of distance between a guessed split and a real split to
			# assume we got the right answer.
			evaluation-tolerance = 4
		}
		
		merger {
			# The maximum ratio between the merged shape candidate's width & x-height to
			# even be considered for merging.
			max-width-ratio = 1.2
			
			# The maximum ratio of the distance between the two inner shapes & the
			# x-height to even be considered for merging.
			max-distance-ratio = 0.15
		}
		
		# The boundary detector type to use when evaluating. Options are:
		# - LetterByLetter: Returns shapes each representing a single letter (after splitting/merging).
		# - Deterministic: Returns a single "most likely" shape boundary guess.
		boundary-detector-type = LetterByLetter
	}
	
	csv {
		# Which character should separate cells in the CSV files generated
		separator = "\t"
	
		# Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
		#encoding = null
		
		# Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
		#locale = null
	}
}